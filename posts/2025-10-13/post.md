I am not a great developer and possibly not even a good developer. However, I have been either learning about or producing software since 2015, and however you slice it, I have accrued plent of experience in the field. That, coupled with some thoughts on AI and the future of software will be the basis of this entry. I won't sugar-coat it, the present and future of software looks bleak. So, button up your flannel, grab some free coffee, and let's jump into cubicle maze of software development to understand why this is. Photos unrelated, but hey, it would be a pretty dry post without them!

![Uji Station](station.jpeg)

# Academic Computer Science
To start at the beginning, universities are not doing a good job of equipping students to become software developers. The bulk of schooling I recieved, for example, focused on the mathematical side of computation on topics like data structures and ["Big O" notation](https://en.wikipedia.org/wiki/Big_O_notation). These are important topics, and I have no qualms with their inclusion into classes on thinking programatically. Later on we focused on software security, another important aspect, and then software design. Again, no qualms. It is not until entering the workforce that I realized what was missing: any kind of project representative of real-world software development as it done in most offices.

The omission is glaring. In school, we are asked to put together small projects of perhaps 500 lines of code each. These explore a topic like optimization, or object oriented code, or program design. Great! Now let's see what I've done in the real world. My first project, which I spent two entire years on, was a fluid pipeline simulation. It started at about 60,000 lines of code. My second project, the one I have been working on alongside 500 other developers for five years, is probably two million lines of code. In my day to day work I spend *very* little time writing code. I spend a whole lot of time reading code, reviewing code, testing code, documenting code, and juggling paperwork about the code. It would maybe have been better to get a degree in documentation and project management *first*, and to have learned programming *second*, as that would reflect better upon my daily work.

In school we make microcosms from scratch. In the real world, businesses need tiny tweaks to gigantic systems that can easily cross into scales of millions of SLOC. That first task assigned to a recent software graduate in a very large and very poorly written legacy application is sobering.

![An alley in Kyoto](alley.jpeg)

# Disarray
There is another trend in commercial software which has been noted as far back as the 80s in [No Silver Bullet](https://en.wikipedia.org/wiki/No_Silver_Bullet), and was succinctly reiterated by the 2006 paper [*Out Of the Tarpit*](https://curtclifton.net/papers/MoseleyMarks06a.pdf). The synopsis can be put forth like this: almost all programs are of very low quality, excessively confusing, and extremely difficult to work with. The end result is bad software delivered late and over-budget. Both papers agree; there is no shortcut or paradigm. The best way out is to keep things as simple as possible, keep the team size as small as possible, and to provide massive padding to timetables. Software is difficult, and it always will be.

There are a few related axioms we can tack on to this: every bit of [state](https://en.wikipedia.org/wiki/State_(computer_science)) in a program doubles the possible states it can be in. For the unintiated, state refers to a setting that the user or another piece of code can change. In a simple system like a lamp, there are two states: on, and off. In a complex system like Microsoft Windows or iOS, there are billions. Predicting how these states interact and transition between other states is very difficult, and failing to handle every state correctly is one way to end up with strange [edge cases](https://en.wikipedia.org/wiki/Edge_case), or places where the text is illegible, or system crashes.

I believe there is another force at work which I haven't found a pithy name for yet on the internet. Let's call it the "shroud principal". It goes something like this: on a production line, it is obvious when things are in disarray. the conveyor belt isn't moving, or the pieces don't fit together, or maybe something is even on fire. On a software floor, there is no indication at all to the casual observer that things are a mess. The code could very bad, or crashing, or even failing to compile, which is the most basic litmus test of code quality, and it is entirely possible for this issue to go unnoticed and uncorrected for days with many developers assuming there is some problem with their local machine.

Somewhat unintuitively, the most expensive programs tend to be of the lowest quality. These are typically large, enterprise systems, written to exacting specifications, requested by one multinational corporation and delivered to another over-schedule and over-budget. Their internals are hashed out in committee meetings, guided by systems reviews, sieved through code reviews, and and honed with thousands of hours of manual testing and millions of automated testing runs. And the end result is they *suck*. They suck because nobody wakes up in the morning and cares enough about the product or has enough individual say in the product to make it cohesive.

On the other hand we have some incredible cases of fantastic free software available to us. In fact, I am writing this from an operating system that is entirely free -- millions upon millions of lines of code -- at no cost. The reason is clear to me. It is easier to get people to take care with the code when they have a personal stake in the product. Nobody wants to help generate money for a shareholder. But providing an incredible service to the world for free? That is something worth doing.

![Arashiyama](bamboo.jpeg)

# Architecture Astronauts
Let's take a higher view of the problem for a bit. So high we leave orbit -- we must discuss the [architecture astronauts](https://www.joelonsoftware.com/2001/04/21/dont-let-architecture-astronauts-scare-you/). You should really read Joel's link to the left -- but if you want my second-hand description, here goes. The astronaut is a sympathetic creature. Their compulsions are understandable. It begins like this: you have two pieces of code that are doing the same task. *Tsk tsk*, you say, so inefficient! I will [refactor](https://en.wikipedia.org/wiki/Code_refactoring) these chunks into a single function. Hmm, you say, there are other ways of adding things. We also add text, and add to collections, and maybe I should have a single function called 'add()' that can handle all of them. That would be great! Ok, so now we just need to define interfaces for these modules that do the adding, and probably some middleware to translate this new adding paradigm to external libraries, and... 

Somewhere along this train of thought you should have realized it crossed over into absurdity. You will find yourself staring at a piece of code with a name like *AbstractNumberInterfaceAdapterFactory* and that should be your cue to stop the insanity and return to writing code that is at most one or two levels of [abstraction](https://en.wikipedia.org/wiki/Abstraction) away from reality, or your model, or whatever it is supposed to actually be *doing* in the real world. However, this doesn't always happen. Sometimes, you continue along this track to nowhere and you sound rather smart doing it so you end up in charge of a project, or running an software architecture review board, and suddenly everyone has to use your insane adding paradigm instead of just adding the numbers together like they actually wanted to.

It sounds laughable. Surely nobody falls for this trap. I can assure you, however, that it is a real phenomenon that is costing actual companies billions of dollars *today*.

![Yamazakicho, Kyoto](buildings.jpeg)

# Alphabet Boys
Next we will look at the 'alphabet boys'. Not the feds, but rather the cottage industry of proscribed software patterns hawked by types like Martin Fowler. This goes hand-in-hand with the Architecture Astronaut who proclaims there is only way to do things. As an exercise, try to wrap your mind around acronyms like [SOLID](https://en.wikipedia.org/wiki/SOLID), [MVVM](https://en.wikipedia.org/wiki/Model%E2%80%93view%E2%80%93viewmodel), or [AOP](https://en.wikipedia.org/wiki/Aspect-oriented_programming). This tendency towards complex patterns described by acronyms lends their name; I don't reject them entirely, but I *do* reject the dogmatic use of one or the other, or insisting on using them where a simpler pattern would suffice.

# Smaug on the garbage heap
I'm not much of a fantasy buff, but let's take a minute to examine Smaug. In *The Hobbit*, Smaug is a jealous dragon sitting atop a pile of gold. He does not let anyone touch his gold, lest they steal it. In software we also have a Smaug. The software version of this is a series of gates that a [code change](https://en.wikipedia.org/wiki/Distributed_version_control#Pull_requests) must pass through to be accepted by whatever server holds the latest copy of code that we're all collectively working on. These changes can include passing automated tests, peer-code review, and enforced documentation. This is not intrinsically a bad thing: it stops the intern from accidentally breaking things, for example. 

However let's revisit this scenario for the case that your master copy of code is of low quality. It needs extensive changes, and a developer who desires to clean up the tarpit, must now fight Smaug, and by extension your coworkers, for every change. The bureaucracy surrounding code becomes your enemy as you're typically only allowed to change the code when a reason has been provided to do so. This protection mechanism should be disabled in such cases, but rarely is this done. The jealous dragon guards his gold, not understanding that it is actually trash.

![Fushimi Inari, Kyoto](shinto.jpeg)

# Electron
This is a relatively new phenomenon. In the past, applications were usually written in at most one or two languages. Now, it is common practice to involve many, and this incidental complexity exacerbates the above issues. One driver of this increase in per-project software languages is the [Electron](https://en.wikipedia.org/wiki/Electron_(software_framework&#41;) framework. It came on to the scene in the early 2010s, and I remember my first experience with the Atom text editor being pleasant. However, of late, I see this pattern used to quickly scaffold poor-quality software, and it incurs a significant performance overhead. The gist of Electron is that it is easier to write websites than proper '[front-ends](https://en.wikipedia.org/wiki/Front_end_and_back_end), so let's just do the UI as a website and ship an entire web browser to the user! 

If done poorly, however, it is noticable to the user. You can tell when you're interacting with a web browser either through degraded performance or because of poor website quality. These are bugs like the screen scrolling to the right or left when it ought to be fixed in place, or the font being a dramatically wrong size, or failing to handle 'backgrounding' well and clearing your entered text just because you closed the application for a minute. 

# Scrumology
This factor goes way back in time. In 2007, a group of developers frustrated with the paradigm of [waterfall](https://en.wikipedia.org/wiki/Waterfall_model)-style software development proposed an alternative in the so-called [Agile Manifesto](https://agilemanifesto.org/). I don't have any actual nits with the Agile Manifesto except to say that it is not well-suited to all types of software. At a minimum it is better than having no SDLC structure at all. However the industry took this concept and fused it with [Scrum](https://en.wikipedia.org/wiki/Scrum_(software_development&#41;), another pattern that goes way back to the 80s and attempts enforce some structure. There is tendency towards useless rituals, bike-shedding meetings, and metric-tracking associated with these patterns that is distinctly unhelpful. For example, my company's SDLC says that any task you take on must be complete within two weeks. Technically we can break things into multiple two week "sprints", but when you're working on something complicated and time-consuming two weeks is often not enough, and "splitting" the work across user stories becomes a tedius behavior that serves only to fracture your work into pieces that are harder to track.

# Large Language Models
I don't have much to say on LLMs because I am not a machine learning researcher. As far as I see it, there are two possibilities for LLMs. We have either reached a dead-end for a very expensive technology that will scale no further. At present, they are somewhat competent 'developers' in the case of small or isolated changes. They cannot develop and deploy a product on their own, though many CEOs would love to convince you they can. The other alternative is that they will continue to improve and free us of the laborious tedium of commercial software development. I am extremely skeptical of this, but I would love to be wrong.


![Kenrokuen, Kanazawa](moss.jpeg)

# Conclusions
I align with the authors of *Out of the Tarpit*. There is only one way out: reduce scope and complexity as much as possible, avoid dogma practice, and stay as close to reality as possible. Avoid introducing new languages or software layers unless absolutely necessary. Do not introduce the overhead of shipping a browser unless absolutely necessary. *Commit* to your platform; don't develop a web application and then ship it to iOS. Keep teams small and lean. The utility of a new developer diminishes rapidly as team size grows. Lastly, we must also just accept that software is difficult. It is best to present your best estimate on software timeline at project outset... and then triple that estimate. [Pareto's Princial](https://en.wikipedia.org/wiki/Pareto_principle) is in effect; the last 20% of the work will take 80% of your timeline.