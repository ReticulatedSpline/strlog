I am not a great developer and possibly not even a good developer. I have also either been learning about or producing software since 2015, so however you slice it I have accrued plenty of experience in the field. That, coupled with some thoughts on AI and the future of software will be the basis of this entry. I won't sugar-coat it, the present and future of software looks bleak. So, button up your flannel, grab some free coffee, and let's jump into cubicle maze of software development to understand why this is. Photos from a recent trip to Japan are unrelated, but it would be a pretty dry post without them!

![Uji Station. The surrounding areas exuberantly celebrate tea culture. Did you know matcha is sometimes added to savory foods like pork dumplings?](station.jpeg)

# Academic Computer Science
To start at the beginning, universities are not doing a good job of equipping students to become software developers. The bulk of schooling I recieved, for example, focused on the mathematical side of computation on topics like data structures and ["Big O" notation](https://en.wikipedia.org/wiki/Big_O_notation). These are important topics, and I have no qualms with their inclusion into classes on thinking programatically. Later on we focused on software security, another important aspect, and then software design. Again, no qualms. It is not until entering the workforce that I realized what was missing: any kind of project representative of real-world software development as it done in most offices.

The omission is glaring. In school, we are asked to put together small projects of perhaps 500 lines of code each. These explore a topic like optimization, or object oriented code, or program design. Great! Now let's see what I've done in the real world. My first project, which I spent two entire years on, was a fluid pipeline simulation. It started at about 60,000 lines of code. My second project, the one I have been working on alongside 500 other developers for five years, is probably two million lines of code. In my day to day work I spend *very* little time writing code. I spend a whole lot of time reading code, reviewing code, testing code, documenting code, and juggling paperwork about the code. It would maybe have been better to get a degree in documentation and project management *first*, and to have learned programming *second*, as that would reflect better upon my daily work.

In school we make microcosms from scratch. In the real world, businesses need tiny tweaks to gigantic systems that can easily cross into scales of millions of SLOC. That first task assigned to a recent software graduate in a very large and very poorly written legacy application is sobering.

![An alley in Kyoto behind our swanky but microscopic hotel.](alley.jpeg)

# Disarray
There is another trend in commercial software which has been noted as far back as the 80s in [No Silver Bullet](https://en.wikipedia.org/wiki/No_Silver_Bullet), and was succinctly reiterated by the 2006 paper [*Out Of the Tarpit*](https://curtclifton.net/papers/MoseleyMarks06a.pdf). The synopsis can be put forth like this: almost all programs are of very low quality, excessively confusing, and extremely difficult to work with. The end result is bad software delivered late and over-budget. Both papers agree; there is no shortcut or panacea paradigm, though many people will happily sell you one. 

There are a few related axioms we can tack on to this: every bit of [state](https://en.wikipedia.org/wiki/State_(computer_science&#41;) in a program doubles the possible states it can be in. For the unintiated, state refers to a setting that the user or another piece of code can change. In a simple system like a lamp, there are two states: on, and off. In a complex system like Microsoft Windows or iOS, there are billions. Predicting how these states will interact and transition between other states is very difficult, and failing to handle every state correctly is one way to end up with strange [edge cases](https://en.wikipedia.org/wiki/Edge_case), or places where the text is illegible, or system crashes.

I believe there is another force at work which I haven't found a pithy name for yet on the internet. Let's call it the "shroud principal". It goes something like this: on a production line, it is obvious when things are in disarray. the conveyor belt isn't moving, or the pieces don't fit together, or maybe something is even on fire. On a software floor, there is no indication at all to the casual observer (or product manager) that things are a mess. The code could very bad, or crashing, or even failing to compile, which is the most basic litmus test of code quality, and it is entirely possible for this issue to go unnoticed and uncorrected for days with many developers assuming there is some problem with their local machine.

Somewhat unintuitively, the most expensive programs tend to be of the lowest quality. These are typically large, enterprise systems, written to exacting specifications, requested by one multinational corporation and delivered to another over-schedule and over-budget. Their internals are hashed out in committee meetings, guided by systems reviews, sieved through code reviews, and and honed with thousands of hours of manual testing and millions of automated testing runs. And the end result is they *suck*. They suck because nobody wakes up in the morning and cares enough about the product or has enough individual say in the product to make it cohesive.

On the other hand we have some incredible cases of fantastic free software available to us. In fact, I am writing this from an operating system that is entirely free -- millions upon millions of lines of code -- at no cost. The reason is clear to me. It is easier to get people to take care with the code when they have a personal stake in the product. Nobody wants to help generate money for a shareholder. But providing an incredible service to the world for free? That is something worth doing.

![Arashiyama bamboo forest was beautiful, but crowded even in the shoulder season.](bamboo.jpeg)

# Architecture Astronauts
Let's take a higher view of the problem for a bit. So high we leave orbit -- we must discuss the [architecture astronauts](https://www.joelonsoftware.com/2001/04/21/dont-let-architecture-astronauts-scare-you/). You should really read Joel's link to the left -- but if you want my second-hand description, here goes. The astronaut is a sympathetic creature. Their compulsions are understandable. It begins like this: you have two pieces of code that are doing the same task. *Tsk tsk*, you say, so inefficient! I will [refactor](https://en.wikipedia.org/wiki/Code_refactoring) these chunks into a single function. Hmm, you say, there are other ways of adding things. We also add text, and add to collections, and maybe I should have a single function called 'add()' that can handle all of them. That would be great! Ok, so now we just need to define interfaces for these modules that do the adding, and probably some middleware to translate this new adding paradigm to external libraries, and... 

Somewhere along this train of thought you should have realized it crossed over into absurdity. You will find yourself staring at a piece of code with a name like *AbstractNumberInterfaceAdapterFactory* and that should be your cue to stop the insanity and return to writing code that is at most one or two levels of [abstraction](https://en.wikipedia.org/wiki/Abstraction) away from reality, or your model, or whatever it is supposed to actually be *doing* in the real world. However, this doesn't always happen. Sometimes, you continue along this track to nowhere and you sound rather smart doing it so you end up in charge of a project, or running an software architecture review board, and suddenly everyone has to use your insane adding paradigm instead of just adding the numbers together like they actually wanted to.

It sounds laughable. Surely nobody falls for this trap. I can assure you, however, that it is a real phenomenon that is costing actual companies billions of dollars *today*.

![Another view outside our hotel in Yamazakicho, Kyoto. I was somewhat enamored with this geometric HVAC work.](buildings.jpeg)

Hand in hand with these astronauts are the 'alphabet boys'. Not the feds, but rather the cottage industry of proscribed software patterns hawked by types like Martin Fowler. Like the Architecture Astronaut, each acronym proclaims there is only *one* way to do things. As an exercise, try to wrap your mind around acronyms like [SOLID](https://en.wikipedia.org/wiki/SOLID), [MVVM](https://en.wikipedia.org/wiki/Model%E2%80%93view%E2%80%93viewmodel), or [AOP](https://en.wikipedia.org/wiki/Aspect-oriented_programming). This tendency towards complex patterns described by acronyms lends their name; I don't reject all of them outright, but I *do* reject the dogmatic use of one or the other applied without, or insisting on using them where a simpler pattern would suffice. In my experience it is better to let this incidental complexity of architecture grow and adapt as it is needed in the project, rather than try to enforce it from the top down. Following the emergent needs of the code will help keep that architectural overhead at a minimum, where it ought to be.

# Smaug on the garbage heap
Let's take a minute to consider Smaug. In *The Hobbit*, Smaug is a jealous dragon sitting atop a pile of gold. He does not let anyone touch his gold, lest they steal it. In software we also have a Smaug. The software Smaug is a series of gates that a [code change](https://en.wikipedia.org/wiki/Distributed_version_control#Pull_requests) must pass through to be accepted by whatever server holds the latest copy of code that we're all collectively working on. These changes can include passing automated tests, peer code review, and enforced documentation. This is not intrinsically a bad thing: that small amount of friction can catch some otherwise sneaky bugs or impose patterns where there might otherwise be none. Like any other tool, however, quality gates can also run amok.

Often it is only allowed to submit code changes when a pre-existing ticket has been generated justifying the business need and defining a scope of action slotted within the appropriate quarter and reviewed by the relevant stakeholders. In addition to this, the appropriate code reviewers must be located, tagged, have signed off on the change, and... oh, and after all that the unit tests aren't passing! Huh? A bit more on those tests: unit tests are so named because they test a small unit of code, like a single function. It is common for enforced testing to push developers into writing useless tests simply because they are required, and these tests sometimes mock out so much of the logic as to test tautologies. There is also computational overhead to these. My current project has a suite of hundreds of thousands of unit tests. This sounds great on paper and looks even better on a PowerBI dashboard, but it hides the reality that many of these tests are not useful. I have even seen tests that, when reduced to their actual parts and stripped of fancy syntax, do nothing but test the CPU's ability to add integers together. Worse than useless, these tests can have a long cumulative runtime, so to get a code change together and, you know, *actually merge it*, can take hours. Sometimes developers are pushed into even more dire straits, like writing non-deterministic tests that rely on things like local machine time. This can causes a test that works in North America, for example, to fail in India. 

So, let's revisit this scenario for the case that a codebase is of low quality. Perhaps it is even crashing in the field. It needs extensive changes *today*, but a developer who desires to clean up the tarpit must now fight the quality gates and by extension their coworkers to make *any* changes. The bureaucracy surrounding code becomes the enemy. When the code is a dumpster fire the best course of action is a rapid offensive with this protection mechanism disabled, but this is rarely done. The code gates serve only to protect the garbage, and the jealous dragon guards his gold, not understanding that it is actually trash.

![Fushimi Inari, Kyoto. I might be more inclined to Western eclesiastics if they infused some Japanese playfulness. See also: Matsuchiyama Shoden Honryuin, the Tokyoite temple dedicated to giant radishes.](shinto.jpeg)

# Performance and failure to KISS
Here we will start with a case study in software architecture known as [Electron](https://en.wikipedia.org/wiki/Electron_(software_framework&#41;). Before Electron, applications were usually written in a single language. Now, it is common practice to involve many, and this incidental complexity exacerbates all of the above issues. You may now need to make changes to multiple repositories with multiple sets of quality gates! One driver of this increase in per-project software languages is the Electron framework. It came on to the scene in the early 2010s, and I remember my first experience with the Atom text editor being pleasant. However, of late, I see this pattern used to quickly scaffold poor-quality software, and it incurs a significant performance overhead. The gist of Electron is that it is easier to write websites than proper '[front-ends](https://en.wikipedia.org/wiki/Front_end_and_back_end), so let's just do the UI as a website and ship an entire web browser to the user! 

If done poorly, however, it is noticable to the user. You can tell when you're interacting with a web browser either through degraded performance or because of poor website quality. Next time you end up waiting for a website to process something, consider the fact that most processors work at a speed of billions of operations per second. The fact that we *ever* have to wait for a computer to do *anything* is astonishing. It speaks to the massive scale of waste that poorly implemented software cases. Waste of resources, waste of effort, and waste of time.

Then there are the bugs. Electron style bugs have proliferated all over, and include gems like the screen scrolling to the right or left when it ought to be fixed in place, or the font being a dramatically wrong size, or failing to handle 'backgrounding' well and clearing the text you just entered just because you closed the application for a few seconds. Consider that this also duplicates state -- you are now juggling the state of the front-end web technology along side the back-end state of whatever you wrote your business logic in. This is not novel, and many applications successfully juggle state in this way, but I will posit that splitting state across *languages* is a uniquely complicated task. Those two languages are often augmented by a third location for state like a database or messaging layer, and it often adds up a lot of bugs. Consolidating languages is a great way to reduce surface area for bugs to manifest.

Now I will ask you to forgive me because I am going to reneg on my earlier statements about acronyms. There are actually two I find worthwhile: KISS, and YAGNI. The first stands for "Keep It Simple, Stupid!" This conveneiently underscores my primary complaint with Electron. It isn't simple! Writing a backend in Java but then eschewing the many functional Java front-ends for a totally different JavaScript paradigm is not simple. It adds extra state, extra configuration, and extra confusion. The second is YAGNI, or, "Ya Ain't Gunna Need it." YAGNI is a guiding principle that says we should not develop features for the future, only features we need today. Electron (actually worse, a proprietary version of Electron) was chosen in a past work project of mine which caused all aforementioned problems and then some. The rallying cry of the designers, even years into the boondoggle, was that Electron granted us the freedom to someday run our program on the cloud rather than on-device. For over a decade the project team has dealt with extra complexity and bugs for this hypothetical future scenario. To date there are no plans to execute it.

# Scrumology
Now a brief note on software people processes. This factor goes way back in time. In 2007, a group of developers frustrated with the paradigm of [waterfall](https://en.wikipedia.org/wiki/Waterfall_model)-style software development proposed an alternative in the so-called [Agile Manifesto](https://agilemanifesto.org/). I don't have any actual nits with the Agile Manifesto except to say that it is not well-suited to all types of software. At a minimum it is better than having no [SDLC](https://en.wikipedia.org/wiki/Systems_development_life_cycle) structure at all. However the industry took this concept and fused it with [Scrum](https://en.wikipedia.org/wiki/Scrum_(software_development&#41;), another pattern that goes way back to the 80s and attempts enforce some structure. There is tendency towards useless rituals, bike-shedding meetings, and metric-tracking associated with these patterns that is distinctly unhelpful. For example, my company's SDLC says that any task you take on must be complete within two weeks. Technically we can break things into multiple two week "sprints", but when you're working on something complicated and time-consuming two weeks is often not enough, and "splitting" the work across user stories becomes a tedius behavior that serves only to fracture your work into pieces that are harder to track.

# Large Language Models
I don't have much to say on LLMs because I am not a machine learning researcher. As far as I see it, there are two possibilities for LLMs. We have either reached a dead-end for a very expensive technology that will scale no further. At present, they are somewhat competent 'developers' in the case of small or isolated changes. They cannot develop and deploy a product on their own, though many CEOs would love to convince you they can. The other alternative is that they will continue to improve and free us of the laborious tedium of commercial software development. I am extremely skeptical of this, but I would love to be wrong.


![Kenrokuen, Kanazawa. Some truly lovely moss gardens await any who make the trek.](moss.jpeg)

# Conclusions
I align completely with the authors of *Out of the Tarpit*. There is only one way out: reduce scope and complexity as much as possible, avoid dogmatic practice, and stay as close to reality as possible. Avoid introducing new languages or software layers unless absolutely necessary. Do not introduce the overhead of shipping a browser unless absolutely necessary. *Commit* to your platform; don't develop a web application and then ship it to iOS. Keep teams small and lean. The utility of a new developer diminishes rapidly as team size grows. Lastly, we must also just accept that software is difficult. It is best to make a quality estimate on software timeline at project outset... and then triple that estimate. [Pareto's Princial](https://en.wikipedia.org/wiki/Pareto_principle) is in effect; the last 20% of the work will take 80% of your timeline.